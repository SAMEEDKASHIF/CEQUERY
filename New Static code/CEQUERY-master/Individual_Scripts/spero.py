# -*- coding: utf-8 -*-
"""spero.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nhiWRNiteoe34GqxkZ4ebu3ujVqj1_Ez
"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
#from stanfordcorenlp import StanfordCoreNLP


import xlrd
file_name = "C:\\Users\\Azim\\Desktop\\De-identified student comments.xlsx";

import pandas as pd
df = pd.read_excel(io=file_name)
#print(df.head)

df = df.dropna(axis=0, how="any")

from nltk.tokenize import word_tokenize

positive = pd.read_excel(file_name, sheet_name='POSITIVE')
negative = pd.read_excel(file_name, sheet_name='NEGATIVE')

sent_positive = positive.to_string(index=False)
sent_negative = negative.to_string(index=False)

positive_tokens = word_tokenize(sent_positive)
negative_tokens = word_tokenize(sent_negative)

#import the nltk package
import nltk
#call the nltk downloader
nltk.download('punkt')
from nltk.stem.porter import *

from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
import re

stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))


rows = list()
wholedata = " "

word_token = []

for row in df[['HELPFUL', 'IMPROVE']].iterrows():
    r = row[1]
    data =r.get_values()[0]
    lower = data.lower()

    reg = re.sub('[^A-Za-z0-9]+', ' ', lower)
    word_tokens = sent_tokenize(reg)
    word_token.append(word_tokens)

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer

porter=PorterStemmer()
stemmed_sentences = []
def stemSentence(sentence):
    token_words=word_tokenize(sentence)
    token_words
    stem_sentence=[]
    for word in token_words:
        stem_sentence.append(porter.stem(word))
        stem_sentence.append(" ")
    return "".join(stem_sentence)

for row in word_token:
  x=stemSentence(row[0])
  stemmed_sentences.append(x)

print(stemmed_sentences[0])

from textblob import TextBlob
#!pip install stanfordnlp
#!pip install textblob vadersentiment
#from vadersentiment.vadersentiment import SentimentIntensityAnalyzer

sentence_analysis=[]
for row in stemmed_sentences:
  analysis = TextBlob(row)
  sentence_with_sentiment=[row,analysis.sentiment]
  sentence_analysis.append(sentence_with_sentiment)

#print(sentence_analysis)

pos_count = 0
pos_positive = 0
pos_negative = 0
for row in sentence_analysis:
   pos_count +=1
   if row[1].polarity > 0.5:
      pos_positive += 1

   if row[1].polarity < 0.5:
      pos_negative+=1
print(pos_count)
print(pos_positive)
print(pos_negative)

s = ""
for i in stemmed_sentences:
    s+=i


# line of code to start the stanfordnlp server

#java -mx4g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000

from pycorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('http://localhost:9000')
res = nlp.annotate(s,
                   properties={
                       'annotators': 'sentiment',
                       'outputFormat': 'json',
                       'timeout': 750000,
                   })
for s in res["sentences"]:
    print("%d: '%s': %s %s" % (
        s["index"],
        " ".join([t["word"] for t in s["tokens"]]),
        s["sentimentValue"], s["sentiment"]))