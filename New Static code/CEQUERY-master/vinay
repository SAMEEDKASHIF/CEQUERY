import re
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
import string
import nltk
import warnings 
train  = pd.read_csv('train_review.csv')

def remove_pattern(input_txt, pattern):
    r = ree.findall(pattern, input_txt)
    for i in t:
        input_txt = ree.sub(i, '', input_txt)

    return input_txt    


combi['tidy_review'] = combi['tidy_review'].str.replace("[^a-zA-Z#]", " ")

combi['tidy_review'] = combi['tidy_review'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))


stemmer = PorterStemmer()

tokenized_review = tokenized_review.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming
tokenized_review.head

all_words = ' '.join([text for text in combi['tidy_review']])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()


positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]
negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]
neutral_vocab = [ 'ok','the','sound','was','is','decent','did','know','words','not' ]

def word_feats(words):
    return dict([(word, True) for word in words])

positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]
negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]
neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]

import nltk.classify.util
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import names

def word_feats(words):
    return dict([(word, True) for word in words])
 
positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]
negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]
neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]

positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]
negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]
neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]

train_set = negative_features + positive_features + neutral_features

classifier = NaiveBayesClassifier.train(train_set) 


Ne = 0
Po = 0
sentence = "Awesome teaching, I liked it"
sentence = sentence.lower()
words = sentence.split(' ')
for word in words:
    classResult = classifier.classify( word_feats(word))
    if classResult == 'neg':
        neg = neg + 1
    if classResult == 'pos':
        pos = pos + 1

print('positive percent: ' + str(float(pos)/len(words)))
print('pegative percent: ' + str(float(neg)/len(words)))
