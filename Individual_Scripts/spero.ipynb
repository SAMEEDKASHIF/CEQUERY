{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spero.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu9P6XgaiNW8",
        "colab_type": "code",
        "outputId": "a84623ac-a523-4d21-e134-d58eb30a1a43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import nltk  \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')  \n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQQQnXnlcoj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xlrd\n",
        "file_name = 'De-identified student comments.xlsx'\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_excel(io=file_name)\n",
        "#print(df.head)\n",
        "\n",
        "df = df.dropna(axis=0, how=\"any\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHO5BfoPmYIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "positive = pd.read_excel(file_name, sheet_name='POSITIVE')\n",
        "negative = pd.read_excel(file_name, sheet_name='NEGATIVE')\n",
        "\n",
        "sent_positive = positive.to_string(index=False)\n",
        "sent_negative = negative.to_string(index=False)\n",
        "\n",
        "positive_tokens = word_tokenize(sent_positive)\n",
        "negative_tokens = word_tokenize(sent_negative)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZNku_C5F8ex",
        "colab_type": "code",
        "outputId": "40069072-6825-4d48-f547-f4b9ce54cb6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#import the nltk package\n",
        "import nltk\n",
        "#call the nltk downloader\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import * \n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import sent_tokenize \n",
        "import re\n",
        "  "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieWXzVQCknYd",
        "colab_type": "code",
        "outputId": "864660af-1320-4bcd-db60-d7ba4175d2b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "\n",
        "rows = list()\n",
        "wholedata = \" \"\n",
        "filtered_sentence = []\n",
        "stemmed_sentence = []\n",
        "\n",
        "for row in df[['HELPFUL', 'IMPROVE']].iterrows():\n",
        "    r = row[1]\n",
        "    data =r.get_values()[0]\n",
        "    lower = data.lower()\n",
        "\n",
        "    reg = re.sub('[^A-Za-z0-9]+', ' ', lower) \n",
        "    word_tokens = sent_tokenize(reg) \n",
        "word_tokens"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['please continue teaching such units by inspirational teachers like thor who is very great with teaching such subjects as it is a very important subject and we need all we can to get together with like minds to prevent problem solve terrorism']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsF11jbtsM2v",
        "colab_type": "code",
        "outputId": "15bedc5d-37fd-45da-af67-db28544b12a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter=PorterStemmer()\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "x=stemSentence(word_tokens[0])\n",
        "x\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pleas continu teach such unit by inspir teacher like thor who is veri great with teach such subject as it is a veri import subject and we need all we can to get togeth with like mind to prevent problem solv terror '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTc0R51TH13d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line= \"The titular threat of The Blob has always struck me as the ultimate movie monster: an insatiably hungry, amoeba-like mass able to penetrate virtually any safeguard, capable of--as a doomed doctor chillingly describes it assimilating flesh on contact. Snide comparisons to gelatin be damned, it's a concept with the most devastating of potential consequences, not unlike the grey goo scenario proposed by technological theorists fearful of artificial intelligence run rampant.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H34vSaO51fF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "947c6b58-48f1-4175-9d67-e3cff46c89e4"
      },
      "source": [
        "from textblob import TextBlob\n",
        "!pip install stanfordnlp\n",
        "#!pip install textblob vadersentiment\n",
        "#from vadersentiment.vadersentiment import SentimentIntensityAnalyzer\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanfordnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.7.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.16.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (41.0.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L0tJLtnDlXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "c1826c9d-2492-4fb9-c456-b5d7cb71c684"
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('en')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "y\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: y/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [00:07<00:00, 29.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: y/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHMA5JPUD4Qc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "c26f3aa2-1b64-44f3-a224-b6cd55fa466a"
      },
      "source": [
        "nlp = stanfordnlp.Pipeline(processors = \"tokenize,mwt,lemma,pos\")\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Cannot load model from /root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzgkjieiEKp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "3a86e94e-d820-4899-a8de-3d5b83e9f9fd"
      },
      "source": [
        "doc = nlp(\"\"\"The prospects for Britain’s orderly withdrawal from the European Union on March 29 have receded further, even as MPs rallied to stop a no-deal scenario. An amendment to the draft bill on the termination of London’s membership of the bloc obliges Prime Minister Theresa May to renegotiate her withdrawal agreement with Brussels. A Tory backbencher’s proposal calls on the government to come up with alternatives to the Irish backstop, a central tenet of the deal Britain agreed with the rest of the EU.\"\"\")\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-2a01d17016de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"The prospects for Britain’s orderly withdrawal from the European Union on March 29 have receded further, even as MPs rallied to stop a no-deal scenario. An amendment to the draft bill on the termination of London’s membership of the bloc obliges Prime Minister Theresa May to renegotiate her withdrawal agreement with Brussels. A Tory backbencher’s proposal calls on the government to come up with alternatives to the Irish backstop, a central tenet of the deal Britain agreed with the rest of the EU.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4miWy2x5u1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4330685c-d39a-463c-943e-b9d8a19cbde7"
      },
      "source": [
        "analysis = TextBlob(line)\n",
        "print(analysis.sentiment)\n",
        "#print(analysis.sentences)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment(polarity=-0.1590909090909091, subjectivity=0.6931818181818182)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3mR1H_ZAFuN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SA0J5Ki6CbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7879e088-d3ee-4287-ebd4-8dfc1b9c06ba"
      },
      "source": [
        "pos_count = 0\n",
        "pos_correct = 0\n",
        "\n",
        "with open(\"positive.txt\",\"r\") as f:\n",
        "    for line in f.read().split('\\n'):\n",
        "        analysis = TextBlob(line)\n",
        "        if analysis.sentiment.polarity > 0:\n",
        "            pos_correct += 1\n",
        "        pos_count +=1\n",
        "\n",
        "\n",
        "neg_count = 0\n",
        "neg_correct = 0\n",
        "\n",
        "with open(\"negative.txt\",\"r\") as f:\n",
        "    for line in f.read().split('\\n'):\n",
        "        analysis = TextBlob(line)\n",
        "        if analysis.sentiment.polarity <= 0:\n",
        "            neg_correct += 1\n",
        "        neg_count +=1\n",
        "\n",
        "print(\"Positive accuracy = {}% via {} samples\".format(pos_correct/pos_count*100.0, pos_count))\n",
        "print(\"Negative accuracy = {}% via {} samples\".format(neg_correct/neg_count*100.0, neg_count))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive accuracy = 58.333333333333336% via 132 samples\n",
            "Negative accuracy = 50.39370078740157% via 127 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQT-oeiX7Tk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUZyCJz27U1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}